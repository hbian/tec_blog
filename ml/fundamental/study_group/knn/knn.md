## ML Study Group
首先想感谢木东居士和饼干组织并经营这个小组, 给了自己一个循序渐进而且能够和大家一起学习的机会
(W1_KNN学习)[https://mp.weixin.qq.com/s/AG1CgLHBNA5Lpxg_Myo8IA]

## KNN
K nearest neighbor：K最邻近算法.在处理分类问题时，可以这么理解，获取离该样本最近的K个最近距离的邻居的分类, 通过多数表决来判断该样本的分类.

算法的流程可以梳理如下
* 计算预测对象到训练集中对象距离
* 按距离进行排序，并提取最近的k个对象
* 统计这K个邻居的类别，取频率最高的为预测对象的类别


### 距离
这个算法中的一个关键点就是求样本之间的距离，有如下几种常见距离

* 欧式距离 Euclidean Distance

多维空间中的各个点之间的绝对距离， 每个点在各自维度上的坐标相减， 差值平方后进行就和然后开方：

![欧式距离](http://dl2.iteye.com/upload/attachment/0098/4314/bb71ff05-fe7f-3045-bfc7-1bfad452af9f.png)

可以用勾股定理中的求直角边距离来模拟这个距离在二维时的应用。

* 明可夫斯基距离 Minkowski Distance

明氏距离是欧氏距离的推广，是对多个距离度量公式的概括性的表述。公式如下：

![明氏距离](http://dl2.iteye.com/upload/attachment/0098/4316/9567216c-ffd4-3d7f-a871-f8685a304cdd.png)

其实可以看到欧式距离和曼哈顿距离就是P=2 和 P=1时的情况

* 曼哈顿距离 Manhattan Distance
对距离进行简单估算，可以想象求从一个街区去另外个街区仅走车行道的距离

![曼哈顿距离]http://dl2.iteye.com/upload/attachment/0098/4318/87bb1b15-ee66-34ec-890e-f09a3f7aa1ab.png

以上的距离算法都必须保证各个特征必须在一个度量。比如说2个特征都是距离长度为cm， 这个时候我们可以用欧式距离。但是如果是一个人为样本，重量和身高作为特征，那么欧式距离就失效了。

皮尔逊相关系数，余弦相似度可以处理不同量级这个问题，这里记录下，以后再展开研究。

## KNN 简单实现
例子为通过给定的病例中肿瘤大小和发现时间二个特征，来对未知病例进行分类，良性或者恶性。

[knn_poc](knn_poc.py)

常规的机器学习流程是：
选择机器学习算法 -> 机器学习算法.fit(x_train, y_train) -> 得到模型 -> 模型.predict(x_test)-> 输出结果.
kNN算法没有模型，模型其实就是训练数据集，predict的过程就是求k近邻的过程。
