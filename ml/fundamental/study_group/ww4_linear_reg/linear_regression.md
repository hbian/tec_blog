# 线性回归
WW4的学习内容:
* 简单线性回归：简单线性回归及最小二乘法的数据推导
* 实践：简单线性回归实现及向量化应用
* 多元线性回归：多选线性回归和正规方程解及实现

[模型之母：简单线性回归&最小二乘法](https://mp.weixin.qq.com/s/siFRKWLhGOGJCCMjzB7R7A)

[模型之母：简单线性回归的代码实现](https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247483706&idx=1&sn=5595b2da80c7b062786fa9f1f331f20f&scene=21#wechat_redirect)

[线性回归的评价指标](https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247483749&idx=1&sn=0383d2b98ff1b8e4f38e7e96ced3918a&scene=21#wechat_redirect)

[模型之母：多元线性回归](https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247483817&idx=1&sn=55812b65b676ec3e1dcc1c2bdf3ccb1b&scene=21#wechat_redirect)



## 简单线性回归
首先区分二种类型：
* 分类(Classification): Label为离散型的类型(categorical variable)，如处理垃圾邮件，是否生病，颜色类别等. 典型算法KNN

* 回归(Regression):  Label为连续数值型(continuous numerical variable), 比如房价，股价等

简单线性回归： 简单是指只有一个样本特征，线性是指方程是线性的, 所谓回归是指用方程来模拟变量之间是如何关联的.


## 相关推导

比如说我们要求解玩具产量与成本之间的关系:
```
| 玩具个数 | 成本 |
| --- | --- |
| 10 | 7.7 |
| 10 | 9.87 |
| 11 | 10.87 |
| 12 | 12.18 |
| 13 | 11.43 |
| 14 | 13.36 |
```
如果我们把这些点画到图上的话，其实就是找到**一条直线，最大程度拟合样本的特征和样本之间的标记的关系 也就是找到最佳拟合的直线方程：y = ax + b **

假设我们找到了这个方程，那么对于每个样本Xi, 预测值为Yi=a*xi + b, 直线方程越拟合真实的情况，也就是说所有我们的预测值Yi和真实值yi之间的差距越小， 二者之间的距离可以进行如下表示
$$(Y_i - y_i)^2$$

我们可以理解为通过训练样本集找到a和b， 使得下面的计算结果尽可能的小，那么这就是最拟合的方程
$$\sum_{i=1}^m {(Y_i - y_i)^2}$$
Y_i：预测值
y_i：真实值

将y进行替换后
$$\sum_{i=1}^m {(Y_i - ax_i - b)^2}$$

### 总结

** 建模过程，其实就是找到一个模型，最大程度的拟合我们的训练数据。 在简单线回归问题中，模型就是我们的直线方程：y = ax + b  **

要想最大的拟合，也就是要使没有拟合的部分尽可能的小，换句话说就是损失的部分尽可能的小，就是损失函数(loss function),也有算法是衡量拟合的程度，称函数为效用函数utility function。 这里我们的损失函数就是:
$$\sum_{i=1}^m {(Y_i - ax_i - b)^2}$$

近乎所有参数学习算法都是这样的套路：
* 通过分析问题，确定问题的损失函数或者效用函数；
* 然后通过最优化损失函数或者效用函数，获得机器学习的模型

简单线性回归问题，目标就是:

已知训练集x,y 找到使损失函数
$$\sum_{i=1}^m {(Y_i - ax_i - b)^2}$$
最小的a和b

### 损失函数

在机器学习中，所有的算法模型其实都依赖于最小化或最大化某一个函数，我们称之为"目标函数". 最小化的这组函数被称为“损失函数”
** 损失函数描述了单个样本预测值和真实值之间误差的程度。用来度量模型一次预测的好坏 **
损失函数是衡量预测模型预测期望结果表现的指标。损失函数越小，模型的鲁棒性越好

常用损失函数有:
* 0-1损失函数：用来表述分类问题，当预测分类错误时，损失函数值为1，正确为0
* 平方损失函数：用来描述回归问题，用来表示连续性变量，为预测值与真实值差值的平方。（误差值越大、惩罚力度越强，也就是对差值敏感）
* 绝对损失函数：用在回归模型，用距离的绝对值来衡量
* 对数损失函数：是预测值Y和条件概率之间的衡量。事实上，该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反

损失函数是针对单个样本的，一个训练集中存在n个样本，就需要风险函数进行衡量

### 经验风险
经验风险/经验损失：模型关于训经验风险或经验损失练集的平均损失（每个样本的损失加起来，然后平均一下）
经验风险最小的模型为最优模型。在训练集上最小经验风险最小，也就意味着预测值和真实值尽可能接近，模型的效果越好。

### 期望风险
对所有样本（包含未知样本和已知的训练样本）的预测能力，是全局概念。（经验风险则是局部概念，仅仅表示决策函数对训练数据集里的样本的预测能力。理想的模型（决策）函数应该是让所有的样本的损失函数最小（即期望风险最小化）。但是期望风险函数往往不可得，所以用局部最优代替全局最优。这就是经验风险最小化的理论基础。

经验风险和期望风险之间的关系：

经验风险是局部的，基于训练集所有样本点损失函数最小化。经验风险是局部最优，是现实的可求的。

期望风险是全局的，基于所有样本点损失函数最小化。期望风险是全局最优，是理想化的不可求的。

只考虑经验风险的话，会出现过拟合现象，即模型f(x)对训练集中所有的样本点都有最好的预测能力，但是对于非训练集中的样本数据，模型的预测能力非常不好。怎么办？这就需要结构风险。

### 结构风险
对经验风险和期望风险的折中，在经验风险函数后面加一个正则化项（惩罚项），是一个大于0的系数lamada。J(f)表示的是模型的复杂度。

## 线性回归的评价指标

对于训练数据集合来说，使  
$$\sum_{i=1}^m {(Ytrain_i - ytrain_i)^2}$$ 尽可能小

在得到a和b之后将. 可以使用测试集衡量回归算法好坏的标准 
$$\sum_{i=1}^m {(Ytest_i - ytest_i)^2}$$。

### 均方误差MSE
将以上的求和结果再除以m，获得平均值就可以消除数据量带来的差别
Mean Squared Error

$$\frac{1}\{m}\sum_{i=1}^m {(Ytest_i - ytest_i)^2}$$

### 均方根误差RMSE
但是使用均方误差MSE收到量纲的影响，可以将其开方（为了解决方差的量纲问题，将其开方得到平方差）得到均方根误差RMSE（Root Mean Squarde Error）
$$\sqrt{\frac{1}\{m}\sum_{i=1}^m {(Ytest_i - ytest_i)^2}} $$

### 平均绝对误差MAE
对于线性回归算法还有另外一种非常朴素评测标准。要求真实值  与 预测结果  之间的距离最小，可以直接相减做绝对值，加m次再除以m，即可求出平均距离，被称作平均绝对误差MAE（Mean Absolute Error）

### R square
[最好的评价线性回归的指标-R Squared](https://blog.csdn.net/huobumingbai1234/article/details/81041699)

RMSE这样的算法无法进行不同量纲之间的比较，比如比较一个衡量房价的的方差金额一个预测身高的方差。这时候我们需要将这些指标放在[0,1]的相同区间内才更好比较，公式直接在网上搜索。

* 分子： 我们的模型预测产生的错误，
* 分母： 不考虑x的取值，只是很生硬的将所有的预测样本的预测结果都认为是样本y的均值，使用这个模型所产生的错误. 可以理解为一个预测模型，只是该模型与x无关，在机器学习领域称这种模型为基准模型（Baseline Model），适用于所有的线型回归算法

得到如下结论
* R^2 <= 1
* R2越大也好，越大说明减数的分子小，错误率低, 我们的模型越好. 当我们预测模型不犯任何错误时，R2最大值1
* 如果R^2 < 0，说明我们学习到的模型还不如基准模型。此时，很有可能我们的数据不存在任何线性关系。

linear_regression简单代码实现:
[linear_regression_poc](https://github.com/hbian/tec_blog/blob/master/ml/fundamental/study_group/ww4_linear_reg/linear_regression.py)