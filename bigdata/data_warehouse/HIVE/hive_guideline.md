# Hive程序相关规范
## 开发规范
* 单条SQL长度不宜超过一屏。
* SQL子查询嵌套不宜超过3层。
* 少用或者不用Hint，特别是在Hive 2.0后，增强HiveSQL对于成本调优（CBO）的支持，在业务环境变化时可能会导致Hive无法选用最优的执行计划。
*避免SQL代码的复制、粘贴。如果有多处逻辑一致的代码，可以将执行结果存储到临时表中。
* 尽可能使用SQL 自带的高级命令做操作。例如，在多维统计分析中使用cube、groupingset和rollup等命令去替代多个SQL子句的union all。
* 使用set命令，进行配置属性的更改，要有注释。
* 代码里面不允许包含对表/分区/列的DDL语句，除了新增和删除分区。
* Hive SQL 更加适合处理多条数据组合的数据集，不适合处理单条数据，且单条数据之间存在顺序依赖等逻辑关系。例如，有A、B、C 3行数据，当A符合某种条件才能处理B行时，只有A、B符合某种条件，才能处理C行。
* 保持一个查询语句所处理的表类型单一。例如，一个SQL语句中的表都是ORC类型的表，或者都是Parquet表。
* 关注NULL值的数据处理。
* SQL表连接的条件列和查询的过滤列最好要有分区列和分桶列。
* 存在多层嵌套，内层嵌套表的过滤条件不要写到外层，例如：
```
#Wrong
select a.* from a
left join b
on a.id=b.id
where a.no = 1

#To fix
select a.* from (
select * from a where a.no = 1
) a left join b
on a.id=b.id


```

## 设计规范
* 表结构要有注释。
* 列等属性字段需要有注释。
* 尽量不要使用索引。在传统关系型数据库中，通过索引可以快速获取少部分数据，这个阀值一般是10%以内。但在Hive的运用场景中，经常需要批量处理大量数据，且Hive 索引在表和分区有数据更新时不会自动维护，需要手动触发，使用不便。如果查询的字段不在索引中，则会导致整个作业效率更加低下。索引在Hive 3.0后被废弃，使用物化视图或者数据存储采用ORC格式可以替代索引的功能。
* 创建内部表（托管表）不允许指定数据存储路径，一般由集群的管理人员统一规划一个目录并固化在配置中，使用人员只需要使用默认的路径即可。
* 创建非接口表，只允许使用Orc 或者Parquet，同一个库内只运行使用一种数据存储格式。接口表指代与其他系统进行交互的数据表，例如从其他系统导入Hive 时暂时存储的表，或者数据计算完成后提供给其他系统使用的输出表。
* Hive适合处理宽边（列数多的表），适当的冗余有助于Hive的处理性能。
* 表的文件块大小要与HDFS的数据块大小大致相等。

## 命名规范
* 库/表/字段命名要自成一套体系：
* 表以tb_开头
* 临时表以tmp_开头。
* 视图以v_开头。
* 自定义函数以udf_卡头。
* 原始数据所在的库以db_org_开头，明细数据所在库以db_detail_开头，数据仓库以db_dw_开头。

### Hive架构
客户端提交SQL作业到HiveServer2, HiveServer2会根据用户提交的SQL 作业及数据库中现有的元数据信息生成一份可供计算引擎执行的计划。每个执行计划对应若干MapReduce作业，Hive会将所有的MapReduce作业都一一提交到YARN中，由YARN去负责创建MapReduce作业对应的子任务任务，并协调它们的运行。YARN创建的子任务会与HDFS进行交互，获取计算所需的数据，计算完成后将最终的结果写入HDFS或者本地。
